<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <title>Blog | Kate Heise</title>
  <link rel="stylesheet" href="style.css" />
</head>

<body class="blog-page">

  <div class="container">

    <!-- Header Navigation -->
    <nav>
      <ul>
        <li><a href="index.html">Home</a></li>
        <li><a href="projects.html">Projects</a></li>
        <li><a href="blog.html" class="active">Blog</a></li>
        <li><a href="about.html">About</a></li>
        <li><a href="resume.pdf" target="_blank" rel="noopener">Resume</a></li>
      </ul>
    </nav>

    <!-- Blog Layout with Sidebar -->
    <div class="blog-layout">
      <aside class="sidebar-nav">
        <h3>Blog Posts</h3>
        <ul>
          <li><a href="#automation-not-delegation">Automation Is Not Delegation</a></li>
          <li><a href="#ai-followup">When AI Flags the Problem, Who Talks to the Patient?</a></li>
          <li><a href="#ehrmiddleware">When the EHR Becomes Middleware</a></li>
          <li><a href="#innovationpipeline">Innovation Pipeline</a></li>
          <li><a href="#ai-summit">AI Summit</a></li>
          <li><a href="#stayconf">Stay Conference</a></li>
          <li><a href="#rapid">RAPID Framework</a></li>
        </ul>
      </aside>

      <div class="blog-content">
        <section>
          <h1>Blog</h1>
          <p>Short reflections, insights, and ideas from the intersection of critical care, technology, and innovation.</p>
        </section>

        <hr style="border: 0.5px solid #ddd; margin: 2em 0;">

        <!-- NEW POST -->
        <article class="blog-post" id="automation-not-delegation">
          <header class="post-header">
            <h2>Automation Is Not Delegation</h2>
            <p class="post-meta">Jan 2026 • Digital Health • Leadership • Patient Safety</p>
          </header>

          <img
            src="images/roi.jpg"
            alt="Clinician reviewing patient data at bedside"
            class="blog-hero"
          />

          <p><strong>Every conversation about AI in healthcare eventually lands on the same question:</strong></p>
          <p><strong>What’s the ROI?</strong></p>

          <p>It’s a fair question—but it’s also an incomplete one.</p>

          <p>
            Many of the outcomes we care most about don’t show up cleanly in a spreadsheet:
            prevented harm, earlier recognition, cognitive relief, trust. When those things work,
            nothing happens—and that makes them notoriously difficult to quantify.
          </p>

          <p>
            In the rush to demonstrate return, we’ve started to conflate two very different concepts:
            <strong>automation</strong> and <strong>delegation</strong>.
          </p>

          <p><strong>They are not the same.</strong></p>

          <hr />

          <h3>Automation Surfaces Risk. Delegation Assigns Responsibility.</h3>

          <p>
            Automation can be incredibly powerful. Algorithms are excellent at detecting patterns humans miss—subtle
            vital sign trends, gradual lab drift, early signals of deterioration. They can reduce manual work and bring
            the right information to the right person faster.
          </p>

          <p><strong>But automation does not own outcomes.</strong></p>
          <p><strong>Delegation does.</strong></p>

          <p>
            Delegation requires clarity: <em>who</em> is responsible, <em>what</em> they’re responsible for, and <em>when</em> action is expected.
            Automation, when poorly designed, often does the opposite. It surfaces risk without clearly assigning ownership,
            creating the illusion that something is being handled when it isn’t.
          </p>

          <p><strong>The work doesn’t disappear. It just becomes invisible.</strong></p>

          <hr />

          <h3>The Hidden Cost No One Puts in the ROI Deck</h3>

          <p>
            When automation is mistaken for delegation, responsibility fragments.
          </p>

          <p>
            Follow-up becomes assumed rather than explicit. Alerts fire, dashboards populate, tasks are technically “created”—
            but no single person is clearly accountable for closing the loop. Over time, this shows up as delayed interventions,
            normalized near-misses, and quiet failures that are hard to trace back to any one decision.
          </p>

          <p>These are real costs. They just don’t come with neat dollar signs.</p>

          <ul>
            <li>Cognitive load shifted onto already stretched clinicians</li>
            <li>Time spent reconciling “what the system showed” with “what actually happened”</li>
            <li>Moral distress when tools flag risk but don’t enable action</li>
            <li>Erosion of trust in systems that feel busy but ineffective</li>
          </ul>

          <p><strong>This is where ROI quietly leaks away.</strong></p>

          <hr />

          <h3>Where ROI Actually Lives (Even If It’s Hard to Measure)</h3>

          <p>
            If we only define ROI as labor replacement or minutes saved, we miss the point.
          </p>

          <p>The real returns of well-designed automation show up as:</p>

          <ul>
            <li>Fewer delayed escalations</li>
            <li>Clearer handoffs</li>
            <li>Stronger accountability</li>
            <li>Faster learning loops</li>
            <li>Safer systems over time</li>
          </ul>

          <p>
            These returns compound. They improve outcomes, team functioning, and trust.
            They just resist simple quantification.
          </p>

          <p><strong>The absence of a clean metric does not mean the absence of value.</strong></p>

          <hr />

          <h3>A Better Question Than “What’s the ROI?”</h3>

          <p>Instead of asking what work automation replaces, we should ask:</p>

          <ul>
            <li>Who owns the outcome after the alert fires?</li>
            <li>What happens next—and is that explicit?</li>
            <li>Does this system make humans better at their job, or just busier?</li>
          </ul>

          <p>
            Automation should <strong>clarify responsibility</strong>, not obscure it.
          </p>

          <p><strong>If no one owns the outcome, the system doesn’t work—no matter how advanced the algorithm.</strong></p>
        </article>

        <!-- New Post: When AI Flags the Problem -->
        <section class="blog-post" id="ai-followup">
          <h2>When AI Flags the Problem, Who Talks to the Patient?</h2>

          <img
            src="assets/provider_and_patient.jpg"
            alt="AI follow-up and patient communication"
            class="blog-banner"
          />

          <p><em>January 2026</em></p>

          <p>
            AI can tell us <em>who</em> needs attention.
          </p>

          <p>
            It can flag a rising blood pressure trend, a sudden weight change, a drifting glucose value, or a subtle pattern that suggests someone is quietly heading in the wrong direction. Across health systems globally, these capabilities are expanding rapidly—often faster than the workflows designed to respond to them.
          </p>

          <p>
            But AI doesn’t talk to patients.
          </p>

          <p>
            And increasingly, <strong>healthcare providers don’t either</strong>—at least not in the ways that build understanding outside structured encounters.
          </p>

          <h3>A Communication Gap That Crosses Borders</h3>

          <p>
            This is not a failure of individuals or professions. It is a <strong>systems-level shift</strong> happening worldwide.
          </p>

          <p>
            As care becomes more digitized, follow-up conversations are delegated, scripted, automated, or removed entirely. Non-licensed staff are often tasked with outreach without full clinical context. Meanwhile, providers spend more time interpreting dashboards than hearing lived experience.
          </p>

          <p>
            The result is a growing gap between <strong>what the data says</strong> and <strong>what life looks like</strong> for the patient.
          </p>

          <p>
            The good news: because this is a global problem, it’s also a <strong>global opportunity</strong>.
          </p>

          <h3>What If Learners Closed the Loop?</h3>

          <p>
            Here’s a design idea worth considering:
            <strong>embed healthcare learners—students, residents, trainees—into AI-triggered follow-up workflows.</strong>
          </p>

          <p>
            Not as observers. Not as clerical support. As active participants in closing the loop between signal and story.
          </p>

          <p>
            Imagine a workflow like this:
          </p>

          <ul>
            <li>AI flags a patient based on a concerning trend.</li>
            <li>A learner reviews the data <em>and</em> the context.</li>
            <li>The learner contacts the patient directly.</li>
            <li>They ask open-ended questions: what changed, what’s been hard, what doesn’t make sense, what barriers are real.</li>
            <li>They document insights no algorithm can generate.</li>
          </ul>

          <p>
            This approach scales globally because the principle is universal:
            <strong>data identifies risk; humans understand meaning.</strong>
          </p>

          <h3>Designing AI With Learners Is a Global Imperative</h3>

          <p>
            If AI is to be used safely and effectively, learners must be trained <strong>with the tools they will inherit</strong>, not shielded from them.
          </p>

          <p>
            We need to keep critical thinking at the forefront—by exposing learners to AI outputs and limitations, teaching them to challenge confident answers, and making uncertainty visible instead of hidden.
          </p>

          <p>
            The risk isn’t that AI gets better. The risk is that we stop practicing the very thinking we need when systems fail quietly.
          </p>

          <blockquote>
            <em>“If AI flags the problem, humans still have to close the loop.”</em>
          </blockquote>

          <p>
            AI should expand humanity, not compress it. If technology can surface risk faster, then people should spend more time talking, listening, and understanding how real life interferes with ideal plans.
          </p>

          <p class="project-tags">AI • Patient Safety • Medical Education • Human-Centered Design</p>
        </section>

        <!-- New Post: When the EHR Becomes Middleware -->
        <section class="blog-post" id="ehrmiddleware">
          <h2>When the EHR Becomes Middleware: The Future of Clinical Intelligence</h2>
          <img
            src="assets/A_digital_graphic_visually_exploring_the_concept_o.png"
            alt="When the EHR Becomes Middleware"
            class="blog-banner"
          />
          <p><em>October 2025</em></p>
          <p>
            As artificial intelligence evolves, the electronic health record (EHR) will no longer serve as the <em>primary interface</em> between clinicians and data — it will become the <strong>middleware</strong> that connects everything else.
          </p>
          <p>
            Today, clinicians spend too much time documenting, searching, and reconciling information that systems should already know. But in the near future, documentation will be <strong>ambient</strong> — captured automatically through voice, motion, and contextual awareness during patient interactions. AI will recognize not only words, but <strong>intent</strong>, <strong>emotion</strong>, and <strong>physiologic signals</strong>, building a complete narrative of the encounter without interrupting the flow of care.
          </p>
          <p>
            In that model, the EHR becomes a <strong>translator</strong>, not a destination — the engine that harmonizes structured data from wearables, sensors, devices, and human interaction into a coherent clinical record. It becomes invisible, yet indispensable.
          </p>
          <p>
            When that happens, the true measure of a successful digital system won’t be how much we <em>enter</em> into it, but how seamlessly it <em>understands</em> what’s happening in real time.
          </p>
          <p>
            The challenge for healthcare leaders isn’t just building smarter systems — it’s building systems clinicians can trust. Technology should amplify human connection, not replace it. The organizations that succeed will be those that combine ethical AI, thoughtful design, and clinical wisdom into one intelligent, learning infrastructure.
          </p>
          <blockquote>
            <em>“The future of the EHR isn’t documentation. It’s translation.”</em>
          </blockquote>
          <p class="project-tags">AI • EHR Evolution • Clinical Intelligence</p>
        </section>

        <hr style="border: 0.5px solid #ddd; margin: 2em 0;">

        <!-- August 2025 Blog Post -->
        <section class="blog-post" id="innovationpipeline">
          <h2>Bridging Innovation and Implementation: The Hidden Pipeline</h2>
          <p><em>August 2025</em></p>
          <p>
            Every great idea in healthcare innovation starts with a spark — a conversation, a frustration, or a moment of “why can’t we just…”
            But transforming that spark into something measurable, scalable, and sustainable requires more than a good idea. It needs a pipeline.
          </p>
          <p>
            Over the past few years, I’ve seen countless brilliant concepts stall between pilot and practice. It’s not a lack of creativity or data —
            it’s the missing structure that turns innovation into implementation. That’s where I’ve focused much of my work: building frameworks like
            <strong>RAPID</strong> that bridge bedside insight and enterprise systems.
          </p>
          <p>
            The key is closing the loop — connecting those who <em>see</em> the problems with those who can <em>solve</em> them, and ensuring the
            solution doesn’t stop at a single unit, department, or site. That’s where digital platforms, thoughtful data pipelines, and human-centered design converge.
          </p>
          <p>
            If we can master that bridge — the space between ideas and implementation — we can make healthcare innovation not just possible, but predictable.
          </p>
          <blockquote>
            <em>“Innovation isn’t just about creating new things. It’s about making good ideas unavoidable.”</em>
          </blockquote>
          <p class="project-tags">Innovation • RAPID • AMP/CEDAR • Leadership</p>
        </section>

        <!-- AI Summit Post -->
        <section class="blog-post" id="ai-summit">
          <h2>Adaptive Early Warning at the Mayo Clinic AI Summit</h2>
          <p><em>July 2025</em></p>
          <img src="assets/summit.png" alt="Mayo Clinic AI Summit Poster Presentation" class="blog-banner" />
          <p>
            Today I had the opportunity to present my poster at the Mayo Clinic AI Summit on a project close to my heart: using adaptive time frames and volatility metrics to improve early warning systems for patient deterioration.
          </p>
          <p>
            Instead of static thresholds or fixed time windows, I developed a model that adapts based on how often vitals and labs are checked. The goal is to improve real-time alerting and reduce noise while still catching critical changes early.
          </p>
          <p>
            This is part of my ongoing work to build smarter, context-aware clinical tools that actually work for frontline teams — because in healthcare, timing isn't everything… but it's close.
          </p>
          <p>
            It was inspiring to see the energy around healthcare AI today — from predictive models to workflow optimization — and to share space with others passionate about using data responsibly and effectively.
          </p>
          <p>
            I’m especially proud that this project brings together my background in nursing, as a nurse practitioner, operations, and analytics.
          </p>
          <p>
            More to come soon. I’m always up for collaborating on meaningful projects at the intersection of care and computation.
          </p>
          <p class="project-tags">AI Summit • Predictive Modeling • Patient Deterioration</p>
        </section>

        <section class="blog-post" id="stayconf">
          <h2>Why We Built a Conference That Doesn’t Require a Plane Ticket</h2>
          <p><em>December 2024</em></p>
          <p>
            The Critical Care Stay Conference started with a simple idea: what if meaningful professional development didn’t require leaving the hospital, using PTO, or paying hundreds in registration fees?
            We brought that idea to life — and in doing so, created a new kind of learning space grounded in accessibility, relevance, and community.
          </p>
          <p class="project-tags">Equity • Professional Development • Collaboration</p>
        </section>

        <section class="blog-post" id="rapid">
          <h2>What Makes RAPID Work: Innovation from the Bedside Up</h2>
          <p><em>May 2024</em></p>
          <p>
            When we created RAPID, we weren’t just solving problems — we were building a new way to surface them. Frontline staff are often closest to the issues, yet furthest from the decision-making process.
            RAPID gave them a structured, visible, and fast way to elevate insights to leadership — and that changed everything.
          </p>
          <p class="project-tags">Frontline Innovation • Leadership • Systems Change</p>
        </section>

      </div>
    </div>

    <!-- Footer -->
    <footer>
      <hr />
      <nav class="footer-nav">
        <ul>
          <li><a href="index.html">Home</a></li>
          <li><a href="projects.html">Projects</a></li>
          <li><a href="blog.html"
