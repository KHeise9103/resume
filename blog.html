<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <title>Blog | Kate Heise</title>
  <link rel="stylesheet" href="style.css" />
</head>

<body class="blog-page">

  <div class="container">

    <!-- Header Navigation -->
    <nav>
      <ul>
        <li><a href="index.html">Home</a></li>
        <li><a href="projects.html">Projects</a></li>
        <li><a href="blog.html" class="active">Blog</a></li>
        <li><a href="about.html">About</a></li>
        <li><a href="resume.pdf" target="_blank" rel="noopener">Resume</a></li>
      </ul>
    </nav>

    <!-- Blog Layout with Sidebar -->
    <div class="blog-layout">
      <aside class="sidebar-nav">
        <h3>Blog Posts</h3>
        <ul>
          <li><a href="#clinicians-as-partners">From Feedback to Partnership: Engaging Clinicians in AI Development</a></li>
          <li><a href="#post-hype-ai">Past the Hype: Why AI Still Misses the Bedside</a></li>
          <li><a href="#automation-not-delegation">Automation Is Not Delegation</a></li>
          <li><a href="#ai-followup">When AI Flags the Problem, Who Talks to the Patient?</a></li>
          <li><a href="#ehrmiddleware">When the EHR Becomes Middleware</a></li>
          <li><a href="#innovationpipeline">Innovation Pipeline</a></li>
          <li><a href="#ai-summit">AI Summit</a></li>
          <li><a href="#stayconf">Stay Conference</a></li>
          <li><a href="#rapid">RAPID Framework</a></li>
        </ul>
      </aside>

      <div class="blog-content">
        <section>
          <h1>Blog</h1>
          <p>Short reflections, insights, and ideas from the intersection of critical care, technology, and innovation.</p>
        </section>

        <hr style="border: 0.5px solid #ddd; margin: 2em 0;">

        <!-- New Post: From Feedback to Partnership -->
<section class="blog-post" id="clinicians-as-partners">
  <h2>From Feedback to Partnership: Engaging Clinicians in AI Development</h2>

  <p><em>February 2026</em></p>

  <p>
    We’re past the hype phase of AI in healthcare.
  </p>

  <p>
    The technology is real. The models are improving. The investment is there.
    And yet, adoption continues to struggle—not because clinicians resist change,
    but because too many tools are still built <em>around</em> clinical work instead of
    <em>with</em> the people doing it.
  </p>

  <p>
    After a recent conversation sparked a lot of engagement, one question kept coming up:
  </p>

  <blockquote>
    <em>“Okay — so how do we actually do this better?”</em>
  </blockquote>

  <p>
    The answer isn’t another feedback survey, pilot, or usability review at the end of development.
    It requires a structural shift in how we engage clinicians in building technology.
  </p>

  <hr />

  <h3>Stop Treating Clinicians as Reviewers</h3>

  <p>
    In many development cycles, clinicians are asked to “weigh in” after key decisions
    have already been made. By that point, workflows are set, assumptions are baked in,
    and feedback becomes cosmetic rather than transformative.
  </p>

  <p>
    Reviewing a tool is not the same as shaping it.
  </p>

  <p>
    If a solution materially affects bedside work, clinicians shouldn’t just be consulted.
    They should be embedded as partners from the start—present when tradeoffs are discussed,
    scope is defined, and priorities are set.
  </p>

  <p><strong>Engagement only works when clinicians have influence, not just voice.</strong></p>

  <hr />

  <h3>Design for Pressure, Not Policy</h3>

  <p>
    Many AI tools are designed for ideal workflows: uninterrupted time, complete data,
    predictable staffing, and linear decision-making.
  </p>

  <p>
    That’s not how healthcare operates.
  </p>

  <p>
    Real care happens under pressure—at 3 a.m., with competing priorities, incomplete
    information, and constant interruption. If a tool doesn’t work in those conditions,
    it won’t work at all.
  </p>

  <p>
    Development teams should pressure-test designs against reality by asking:
  </p>

  <ul>
    <li>What cognitive task does this remove?</li>
    <li>What new decision does this force?</li>
    <li>What happens if this is ignored?</li>
  </ul>

  <p>
    These questions surface friction early—before it becomes adoption failure.
  </p>

  <hr />

  <h3>Prototype in Real Shifts</h3>

  <p>
    Conference rooms are safe. Clinical environments are not.
  </p>

  <p>
    The fastest way to understand whether a tool fits workflow is to observe how it
    behaves during real shifts. Watch where clinicians hesitate. Watch what they bypass.
    Watch what gets worked around.
  </p>

  <p>
    Every workaround is data.
  </p>

  <p>
    Shadowing live care reveals mismatches between design intent and operational reality
    that no demo can capture.
  </p>

  <hr />

  <h3>Measure What Gets Removed</h3>

  <p>
    Engagement metrics often focus on usage: logins, clicks, alerts fired.
    These numbers say very little about whether work has actually been reduced.
  </p>

  <p>
    More meaningful signals include:
  </p>

  <ul>
    <li>Steps eliminated</li>
    <li>Decisions clarified</li>
    <li>Interruptions avoided</li>
    <li>Time returned to patient care</li>
  </ul>

  <p>
    If a tool adds cognitive load—even subtly—clinicians will feel it immediately,
    long before dashboards reflect a problem.
  </p>

  <hr />

  <h3>Give Frontline Staff Real Stop Power</h3>

  <p>
    Engagement fails when feedback is acknowledged but not acted on.
  </p>

  <p>
    When frontline clinicians say a tool adds work, increases risk, or disrupts care,
    that input should trigger a pause—not a reframe.
  </p>

  <p>
    This doesn’t slow innovation. It prevents expensive failure.
  </p>

  <p>
    Sustainable AI requires psychological safety for clinicians to say,
    <em>“This doesn’t work.”</em> And governance structures that take that seriously.
  </p>

  <hr />

  <h3>The Work That Actually Makes AI Stick</h3>

  <p>
    None of this is theoretical. It’s practical, sometimes uncomfortable,
    and far more effective than the familiar pattern of pilot, rollout,
    and surprise at low adoption.
  </p>

  <p>
    The future of healthcare AI won’t be defined by smarter algorithms alone.
    It will be defined by how well we align technology with the realities of care—
    and whether we treat clinicians as true partners in that work.
  </p>

  <blockquote>
    <em>“If clinicians aren’t part of the build, they will be left carrying the burden.”</em>
  </blockquote>

  <p class="project-tags">Clinical Informatics • AI Implementation • Human-Centered Design • Leadership</p>
</section>

        <!-- New Post: Past the AI Hype -->
<section class="blog-post" id="post-hype-ai">
  <h2>Past the Hype: Why AI Still Misses the Bedside</h2>

  <p><em>January 2026</em></p>

  <p>
    We’re past the AI hype phase in healthcare.
  </p>

  <p>
    At least, we should be.
  </p>

  <p>
    The technology has matured. The investment is real. The promises are everywhere.
    And yet, for many clinicians, the day-to-day experience hasn’t meaningfully improved.
  </p>

  <p>
    The problem isn’t that clinicians “don’t get AI.”
  </p>

  <p><strong>The problem is that too many AI tools don’t get clinicians.</strong></p>

  <hr />

  <h3>The Disconnect No One Likes to Name</h3>

  <p>
    There remains a persistent gap between those building AI tools and those expected to
    use them in real clinical environments.
  </p>

  <p>
    That gap often shows up between:
  </p>

  <ul>
    <li>Developers who have never worked a shift at the bedside</li>
    <li>IT teams optimizing for integration rather than workflow</li>
    <li>Physician AI leads who understand the theory of care but not the reality of nursing work</li>
  </ul>

  <p>
    None of this reflects bad intent. But intent doesn’t offset impact.
  </p>

  <p>
    Meanwhile, bedside staff are asked to absorb yet another tool that promises efficiency—
    while quietly adding cognitive load.
  </p>

  <hr />

  <h3>When AI Lives Outside Workflow</h3>

  <p>
    AI that exists outside real clinical workflows isn’t innovation.
  </p>

  <p><strong>It’s noise.</strong></p>

  <p>
    Dashboards that require extra logins, alerts that interrupt without enabling action,
    and tools that shift work instead of removing it all contribute to the same outcome:
    clinicians disengage.
  </p>

  <p>
    Not because they don’t care—but because they’re already carrying too much.
  </p>

  <hr />

  <h3>What Actually Matters Now</h3>

  <p>
    If we want AI to genuinely improve care, the focus has to shift.
  </p>

  <p>
    It has to:
  </p>

  <ul>
    <li>Reduce work, not repackage it</li>
    <li>Respect how clinicians think under pressure</li>
    <li>Be co-designed with the people doing the work—not “rolled out” to them</li>
  </ul>

  <p>
    These are not technical limitations. They are design and governance choices.
  </p>

  <hr />

  <h3>The Next Phase of Clinical AI</h3>

  <p>
    The future of clinical AI isn’t about smarter models.
  </p>

  <p><strong>It’s about better alignment between technology, people, and practice.</strong></p>

  <p>
    Until that alignment becomes the default—not the exception—healthcare will continue
    to see impressive pilots, polished demos, and disappointing adoption.
  </p>

  <blockquote>
    <em>“AI succeeds at the bedside only when it understands the work happening there.”</em>
  </blockquote>

  <p class="project-tags">AI • Clinical Informatics • Workflow Design • Human-Centered Care</p>
</section>

        <!-- NEW POST -->
        <article class="blog-post" id="automation-not-delegation">
          <header class="post-header">
            <h2>Automation Is Not Delegation</h2>
            <p class="post-meta">Jan 2026 • Digital Health • Leadership • Patient Safety</p>
          </header>

          <img
            src="assets/roi.jpg"
            alt="Clinician reviewing patient data at bedside"
            class="blog-hero"
          />

          <p><strong>Every conversation about AI in healthcare eventually lands on the same question:</strong></p>
          <p><strong>What’s the ROI?</strong></p>

          <p>It’s a fair question—but it’s also an incomplete one.</p>

          <p>
            Many of the outcomes we care most about don’t show up cleanly in a spreadsheet:
            prevented harm, earlier recognition, cognitive relief, trust. When those things work,
            nothing happens—and that makes them notoriously difficult to quantify.
          </p>

          <p>
            In the rush to demonstrate return, we’ve started to conflate two very different concepts:
            <strong>automation</strong> and <strong>delegation</strong>.
          </p>

          <p><strong>They are not the same.</strong></p>

          <hr />

          <h3>Automation Surfaces Risk. Delegation Assigns Responsibility.</h3>

          <p>
            Automation can be incredibly powerful. Algorithms are excellent at detecting patterns humans miss—subtle
            vital sign trends, gradual lab drift, early signals of deterioration. They can reduce manual work and bring
            the right information to the right person faster.
          </p>

          <p><strong>But automation does not own outcomes.</strong></p>
          <p><strong>Delegation does.</strong></p>

          <p>
            Delegation requires clarity: <em>who</em> is responsible, <em>what</em> they’re responsible for, and <em>when</em> action is expected.
            Automation, when poorly designed, often does the opposite. It surfaces risk without clearly assigning ownership,
            creating the illusion that something is being handled when it isn’t.
          </p>

          <p><strong>The work doesn’t disappear. It just becomes invisible.</strong></p>

          <hr />

          <h3>The Hidden Cost No One Puts in the ROI Deck</h3>

          <p>
            When automation is mistaken for delegation, responsibility fragments.
          </p>

          <p>
            Follow-up becomes assumed rather than explicit. Alerts fire, dashboards populate, tasks are technically “created”—
            but no single person is clearly accountable for closing the loop. Over time, this shows up as delayed interventions,
            normalized near-misses, and quiet failures that are hard to trace back to any one decision.
          </p>

          <p>These are real costs. They just don’t come with neat dollar signs.</p>

          <ul>
            <li>Cognitive load shifted onto already stretched clinicians</li>
            <li>Time spent reconciling “what the system showed” with “what actually happened”</li>
            <li>Moral distress when tools flag risk but don’t enable action</li>
            <li>Erosion of trust in systems that feel busy but ineffective</li>
          </ul>

          <p><strong>This is where ROI quietly leaks away.</strong></p>

          <hr />

          <h3>Where ROI Actually Lives (Even If It’s Hard to Measure)</h3>

          <p>
            If we only define ROI as labor replacement or minutes saved, we miss the point.
          </p>

          <p>The real returns of well-designed automation show up as:</p>

          <ul>
            <li>Fewer delayed escalations</li>
            <li>Clearer handoffs</li>
            <li>Stronger accountability</li>
            <li>Faster learning loops</li>
            <li>Safer systems over time</li>
          </ul>

          <p>
            These returns compound. They improve outcomes, team functioning, and trust.
            They just resist simple quantification.
          </p>

          <p><strong>The absence of a clean metric does not mean the absence of value.</strong></p>

          <hr />

          <h3>A Better Question Than “What’s the ROI?”</h3>

          <p>Instead of asking what work automation replaces, we should ask:</p>

          <ul>
            <li>Who owns the outcome after the alert fires?</li>
            <li>What happens next—and is that explicit?</li>
            <li>Does this system make humans better at their job, or just busier?</li>
          </ul>

          <p>
            Automation should <strong>clarify responsibility</strong>, not obscure it.
          </p>

          <p><strong>If no one owns the outcome, the system doesn’t work—no matter how advanced the algorithm.</strong></p>
        </article>

        <!-- New Post: When AI Flags the Problem -->
        <section class="blog-post" id="ai-followup">
          <h2>When AI Flags the Problem, Who Talks to the Patient?</h2>

          <img
            src="assets/provider_and_patient.jpg"
            alt="AI follow-up and patient communication"
            class="blog-banner"
          />

          <p><em>January 2026</em></p>

          <p>
            AI can tell us <em>who</em> needs attention.
          </p>

          <p>
            It can flag a rising blood pressure trend, a sudden weight change, a drifting glucose value, or a subtle pattern that suggests someone is quietly heading in the wrong direction. Across health systems globally, these capabilities are expanding rapidly—often faster than the workflows designed to respond to them.
          </p>

          <p>
            But AI doesn’t talk to patients.
          </p>

          <p>
            And increasingly, <strong>healthcare providers don’t either</strong>—at least not in the ways that build understanding outside structured encounters.
          </p>

          <h3>A Communication Gap That Crosses Borders</h3>

          <p>
            This is not a failure of individuals or professions. It is a <strong>systems-level shift</strong> happening worldwide.
          </p>

          <p>
            As care becomes more digitized, follow-up conversations are delegated, scripted, automated, or removed entirely. Non-licensed staff are often tasked with outreach without full clinical context. Meanwhile, providers spend more time interpreting dashboards than hearing lived experience.
          </p>

          <p>
            The result is a growing gap between <strong>what the data says</strong> and <strong>what life looks like</strong> for the patient.
          </p>

          <p>
            The good news: because this is a global problem, it’s also a <strong>global opportunity</strong>.
          </p>

          <h3>What If Learners Closed the Loop?</h3>

          <p>
            Here’s a design idea worth considering:
            <strong>embed healthcare learners—students, residents, trainees—into AI-triggered follow-up workflows.</strong>
          </p>

          <p>
            Not as observers. Not as clerical support. As active participants in closing the loop between signal and story.
          </p>

          <p>
            Imagine a workflow like this:
          </p>

          <ul>
            <li>AI flags a patient based on a concerning trend.</li>
            <li>A learner reviews the data <em>and</em> the context.</li>
            <li>The learner contacts the patient directly.</li>
            <li>They ask open-ended questions: what changed, what’s been hard, what doesn’t make sense, what barriers are real.</li>
            <li>They document insights no algorithm can generate.</li>
          </ul>

          <p>
            This approach scales globally because the principle is universal:
            <strong>data identifies risk; humans understand meaning.</strong>
          </p>

          <h3>Designing AI With Learners Is a Global Imperative</h3>

          <p>
            If AI is to be used safely and effectively, learners must be trained <strong>with the tools they will inherit</strong>, not shielded from them.
          </p>

          <p>
            We need to keep critical thinking at the forefront—by exposing learners to AI outputs and limitations, teaching them to challenge confident answers, and making uncertainty visible instead of hidden.
          </p>

          <p>
            The risk isn’t that AI gets better. The risk is that we stop practicing the very thinking we need when systems fail quietly.
          </p>

          <blockquote>
            <em>“If AI flags the problem, humans still have to close the loop.”</em>
          </blockquote>

          <p>
            AI should expand humanity, not compress it. If technology can surface risk faster, then people should spend more time talking, listening, and understanding how real life interferes with ideal plans.
          </p>

          <p class="project-tags">AI • Patient Safety • Medical Education • Human-Centered Design</p>
        </section>

        <!-- New Post: When the EHR Becomes Middleware -->
        <section class="blog-post" id="ehrmiddleware">
          <h2>When the EHR Becomes Middleware: The Future of Clinical Intelligence</h2>
          <img
            src="assets/A_digital_graphic_visually_exploring_the_concept_o.png"
            alt="When the EHR Becomes Middleware"
            class="blog-banner"
          />
          <p><em>October 2025</em></p>
          <p>
            As artificial intelligence evolves, the electronic health record (EHR) will no longer serve as the <em>primary interface</em> between clinicians and data — it will become the <strong>middleware</strong> that connects everything else.
          </p>
          <p>
            Today, clinicians spend too much time documenting, searching, and reconciling information that systems should already know. But in the near future, documentation will be <strong>ambient</strong> — captured automatically through voice, motion, and contextual awareness during patient interactions. AI will recognize not only words, but <strong>intent</strong>, <strong>emotion</strong>, and <strong>physiologic signals</strong>, building a complete narrative of the encounter without interrupting the flow of care.
          </p>
          <p>
            In that model, the EHR becomes a <strong>translator</strong>, not a destination — the engine that harmonizes structured data from wearables, sensors, devices, and human interaction into a coherent clinical record. It becomes invisible, yet indispensable.
          </p>
          <p>
            When that happens, the true measure of a successful digital system won’t be how much we <em>enter</em> into it, but how seamlessly it <em>understands</em> what’s happening in real time.
          </p>
          <p>
            The challenge for healthcare leaders isn’t just building smarter systems — it’s building systems clinicians can trust. Technology should amplify human connection, not replace it. The organizations that succeed will be those that combine ethical AI, thoughtful design, and clinical wisdom into one intelligent, learning infrastructure.
          </p>
          <blockquote>
            <em>“The future of the EHR isn’t documentation. It’s translation.”</em>
          </blockquote>
          <p class="project-tags">AI • EHR Evolution • Clinical Intelligence</p>
        </section>

        <hr style="border: 0.5px solid #ddd; margin: 2em 0;">

        <!-- August 2025 Blog Post -->
        <section class="blog-post" id="innovationpipeline">
          <h2>Bridging Innovation and Implementation: The Hidden Pipeline</h2>
          <p><em>August 2025</em></p>
          <p>
            Every great idea in healthcare innovation starts with a spark — a conversation, a frustration, or a moment of “why can’t we just…”
            But transforming that spark into something measurable, scalable, and sustainable requires more than a good idea. It needs a pipeline.
          </p>
          <p>
            Over the past few years, I’ve seen countless brilliant concepts stall between pilot and practice. It’s not a lack of creativity or data —
            it’s the missing structure that turns innovation into implementation. That’s where I’ve focused much of my work: building frameworks like
            <strong>RAPID</strong> that bridge bedside insight and enterprise systems.
          </p>
          <p>
            The key is closing the loop — connecting those who <em>see</em> the problems with those who can <em>solve</em> them, and ensuring the
            solution doesn’t stop at a single unit, department, or site. That’s where digital platforms, thoughtful data pipelines, and human-centered design converge.
          </p>
          <p>
            If we can master that bridge — the space between ideas and implementation — we can make healthcare innovation not just possible, but predictable.
          </p>
          <blockquote>
            <em>“Innovation isn’t just about creating new things. It’s about making good ideas unavoidable.”</em>
          </blockquote>
          <p class="project-tags">Innovation • RAPID • AMP/CEDAR • Leadership</p>
        </section>

        <!-- AI Summit Post -->
        <section class="blog-post" id="ai-summit">
          <h2>Adaptive Early Warning at the Mayo Clinic AI Summit</h2>
          <p><em>July 2025</em></p>
          <img src="assets/summit.png" alt="Mayo Clinic AI Summit Poster Presentation" class="blog-banner" />
          <p>
            Today I had the opportunity to present my poster at the Mayo Clinic AI Summit on a project close to my heart: using adaptive time frames and volatility metrics to improve early warning systems for patient deterioration.
          </p>
          <p>
            Instead of static thresholds or fixed time windows, I developed a model that adapts based on how often vitals and labs are checked. The goal is to improve real-time alerting and reduce noise while still catching critical changes early.
          </p>
          <p>
            This is part of my ongoing work to build smarter, context-aware clinical tools that actually work for frontline teams — because in healthcare, timing isn't everything… but it's close.
          </p>
          <p>
            It was inspiring to see the energy around healthcare AI today — from predictive models to workflow optimization — and to share space with others passionate about using data responsibly and effectively.
          </p>
          <p>
            I’m especially proud that this project brings together my background in nursing, as a nurse practitioner, operations, and analytics.
          </p>
          <p>
            More to come soon. I’m always up for collaborating on meaningful projects at the intersection of care and computation.
          </p>
          <p class="project-tags">AI Summit • Predictive Modeling • Patient Deterioration</p>
        </section>

        <section class="blog-post" id="stayconf">
          <h2>Why We Built a Conference That Doesn’t Require a Plane Ticket</h2>
          <p><em>December 2024</em></p>
          <p>
            The Critical Care Stay Conference started with a simple idea: what if meaningful professional development didn’t require leaving the hospital, using PTO, or paying hundreds in registration fees?
            We brought that idea to life — and in doing so, created a new kind of learning space grounded in accessibility, relevance, and community.
          </p>
          <p class="project-tags">Equity • Professional Development • Collaboration</p>
        </section>

        <section class="blog-post" id="rapid">
          <h2>What Makes RAPID Work: Innovation from the Bedside Up</h2>
          <p><em>May 2024</em></p>
          <p>
            When we created RAPID, we weren’t just solving problems — we were building a new way to surface them. Frontline staff are often closest to the issues, yet furthest from the decision-making process.
            RAPID gave them a structured, visible, and fast way to elevate insights to leadership — and that changed everything.
          </p>
          <p class="project-tags">Frontline Innovation • Leadership • Systems Change</p>
        </section>

      </div>
    </div>


    <!-- Footer -->
    <footer>
      <hr />
      <nav class="footer-nav">
        <ul>
          <li><a href="index.html">Home</a></li>
          <li><a href="projects.html">Projects</a></li>
          <li><a href="blog.html" class="active">Blog</a></li>
          <li><a href="about.html">About</a></li>
          <li><a href="resume.pdf" target="_blank">Resume</a></li>
        </ul>
      </nav>
      <div class="contact-footer">
        <p>Contact: <a href="mailto:kateheise9103@gmail.com">kateheise9103@gmail.com</a></p>
        <p>GitHub: <a href="https://github.com/kheise9103" target="_blank">github.com/kheise9103</a></p>
      </div>
    </footer>

  </div>

  <!-- Back to Top Button -->
  <button id="backToTop" title="Go to top">↑</button>

  <!-- JavaScript for Back to Top -->
  <script>
    const backToTop = document.getElementById("backToTop");

    window.onscroll = function() {
      if (document.body.scrollTop > 400 || document.documentElement.scrollTop > 400) {
        backToTop.classList.add("show");
      } else {
        backToTop.classList.remove("show");
      }
    };

    backToTop.addEventListener("click", () => {
      window.scrollTo({ top: 0, behavior: "smooth" });
    });
  </script>

</body>
</html>



